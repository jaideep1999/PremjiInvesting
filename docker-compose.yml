
# premji_invest_assignment/docker-compose.yml

version: '3.8'

x-airflow-common: &airflow-common
  image: premji-invest-de-assignment:latest # Use the image we build
  build:
    context: .
    dockerfile: Dockerfile
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    - AIRFLOW_UID=50000 # To avoid permission issues
  volumes:
    - ./dags:/opt/airflow/dags
    - ./scripts:/opt/airflow/scripts
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
  depends_on:
    - postgres # Even with sqlite, it's good practice for future scaling

services:
  postgres: # Included for scalability, though we use sqlite for this project
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      timeout: 5s
      retries: 5

  airflow-init:
    <<: *airflow-common
    container_name: airflow_init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # Initialize the database and create an admin user
        airflow db init && \
        airflow users create \
          --username airflow \
          --firstname Airflow \
          --lastname Admin \
          --role Admin \
          --email admin@example.com \
          --password airflow
    restart: on-failure

  airflow-webserver:
    <<: *airflow-common
    command: airflow webserver
    ports:
      - "8080:8080"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    command: airflow scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    restart: always
